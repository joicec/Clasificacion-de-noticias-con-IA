# -*- coding: utf-8 -*-
"""Grupo 3 - Entrenamiento

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14VLL0u8DViNIMaN1dTQljwEeAMtthRjh

# EDA y Entrenamiento del modelo

Comenzamos entendiendo nuestro dataset y analizandolo y haciendo limpieza de nulos. Luego dividimos el dataset en entrenamiento, validacion y test. Normalizamos el texto y sacamos conectores para hacer analisis de palabras.

Importar librerias necescarias
"""

# Importamos dependencias
import pandas as pd
import numpy as np
!pip install openpyxl
import matplotlib.pyplot as plt
import re # Manejo de expresiones regulares
import seaborn as sns
import random
import datetime



from bs4 import BeautifulSoup

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

from gensim.models import Word2Vec, KeyedVectors
import gensim.downloader as api

# Configuración visual para los gráficos
sns.set(style="whitegrid")

# Descargar recursos de NLTK
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

"""# **Carga del Dataset**"""

from google.colab import drive
drive.mount('/content/drive')

# Cargar el dataset
data = "/content/drive/MyDrive/2.dataset_news.xlsx"
dataset_news = pd.read_excel(data)

# Eliminar filas con valores nulos en la columna 'text'  (que es el que tiene la noticia)
dataset_news = dataset_news.dropna(subset=['text'])

"""## Inspección Inicial


"""

# Información general del dataset
dataset_news.info()

# Resumen estadístico de las variables numéricas
dataset_news.describe()

# Conteo de valores faltantes por columna
dataset_news.isnull().sum()

"""## Añadiendo una columna para categorizar la clasificacion de las noticias

1 politic, 2 science, 3 technology

"""

# Añadir columna clasificacion 1 politic, 2 science, 3 technology

# Definir la función personalizada para el mapeo
def label_to_number(label):
    if label == 'politic':
        return 1
    elif label == 'science':
        return 2
    elif label == 'technology':
        return 3
    else:
        return 0  # O cualquier otro valor para etiquetas no esperadas

# Crear la nueva columna en el DataFrame
dataset_news['clasificacion'] = dataset_news['label'].apply(label_to_number)

"""Añadiendo una columna para contar la cantidad de caracteres de las noticias"""

# Calcular la cantidad de caracteres de las noticias
dataset_news['cant_caracteres'] = dataset_news['text'].apply(lambda x: len(x) if pd.notnull(x) else 0)


print(dataset_news.head())

# Mostrar algunas filas del dataset events_df
dataset_news.head(12)

"""## Exploración de Datos



"""

# Crear un DataFrame para facilitar la visualización
news_count_by_type = dataset_news['label'].value_counts()
news_count_df = pd.DataFrame({'Tipo de Noticia': news_count_by_type.index, 'Cantidad': news_count_by_type.values})

# Crear el gráfico de barras
plt.figure(figsize=(10, 6))
sns.barplot(x='Tipo de Noticia', y='Cantidad', data=news_count_df, palette='viridis')
plt.title('Cantidad de Noticias por Tipo de Noticia')
plt.xlabel('Tipo de Noticia')
plt.ylabel('Cantidad de Noticias')
plt.xticks(rotation=45)
plt.show()

dataset_news['date'] = pd.to_datetime(dataset_news['date'])

# Extraer el día de la semana (lunes=0, martes=1, ..., domingo=6)
dataset_news['day_of_week'] = dataset_news['date'].dt.dayofweek

# Mapear el número del día de la semana al nombre del día
day_mapping = {0: 'Lunes', 1: 'Martes', 2: 'Miércoles', 3: 'Jueves', 4: 'Viernes', 5: 'Sábado', 6: 'Domingo'}
dataset_news['day_of_week'] = dataset_news['day_of_week'].map(day_mapping)

# Contar la cantidad de noticias por día de la semana
news_per_day_of_week = dataset_news['day_of_week'].value_counts()

# Ordenar los días de la semana
news_per_day_of_week = news_per_day_of_week.reindex(day_mapping.values())

# Crear el gráfico de barras
plt.figure(figsize=(10, 6))
news_per_day_of_week.plot(kind='bar', color='skyblue')
plt.title('Cantidad de noticias por día de la semana')
plt.xlabel('Día de la semana')
plt.ylabel('Cantidad de noticias')
plt.xticks(rotation=45)
plt.show()

# Contar la cantidad de noticias por día de la semana y tipo de noticia
news_per_day_and_label = dataset_news.groupby(['day_of_week', 'label']).size().unstack(fill_value=0)

# Ordenar los días de la semana
news_per_day_and_label = news_per_day_and_label.reindex(day_mapping.values())

# Crear el gráfico de barras apiladas
plt.figure(figsize=(12, 6))
news_per_day_and_label.plot(kind='bar', stacked=True)
plt.title('Cantidad de noticias por día de la semana y tipo de noticia')
plt.xlabel('Día de la semana')
plt.ylabel('Cantidad de noticias')
plt.xticks(rotation=45)
plt.legend(title='Tipo de noticia')
plt.show()

# Extraer el mes
dataset_news['month'] = dataset_news['date'].dt.month

# Mapear el número del mes al nombre del mes
month_mapping = {1: 'Enero', 2: 'Febrero', 3: 'Marzo', 4: 'Abril', 5: 'Mayo', 6: 'Junio',
                 7: 'Julio', 8: 'Agosto', 9: 'Septiembre', 10: 'Octubre', 11: 'Noviembre', 12: 'Diciembre'}
dataset_news['month'] = dataset_news['month'].map(month_mapping)

# Contar la cantidad de noticias por mes
news_per_month = dataset_news['month'].value_counts()

# Ordenar los meses
news_per_month = news_per_month.reindex(month_mapping.values())

# Crear el gráfico de barras
plt.figure(figsize=(10, 6))
news_per_month.plot(kind='bar', color='skyblue')
plt.title('Cantidad de noticias por mes')
plt.xlabel('Mes')
plt.ylabel('Cantidad de noticias')
plt.xticks(rotation=45)
plt.show()

# Extraer el mes de la fecha
dataset_news['month'] = dataset_news['date'].dt.month

# Mapear el número del mes al nombre del mes
month_mapping = {1: 'Enero', 2: 'Febrero', 3: 'Marzo', 4: 'Abril', 5: 'Mayo', 6: 'Junio',
                 7: 'Julio', 8: 'Agosto', 9: 'Septiembre', 10: 'Octubre', 11: 'Noviembre', 12: 'Diciembre'}
dataset_news['month_name'] = dataset_news['month'].map(month_mapping)

# Agrupar por mes y tipo de noticia, y contar la cantidad de noticias para cada combinación
news_per_month_and_label = dataset_news.groupby(['month_name', 'label']).size().unstack(fill_value=0)

# Crear el gráfico de barras agrupadas
plt.figure(figsize=(12, 6))
news_per_month_and_label.plot(kind='bar')
plt.title('Cantidad de noticias por mes dividido por tipo de noticia')
plt.xlabel('Mes')
plt.ylabel('Cantidad de noticias')
plt.xticks(rotation=45)
plt.legend(title='Tipo de noticia')
plt.show()

"""**Cantidad de noticias por mes y tipo**

## Detección de Outliers y Imputación de Valores Faltantes

Identificar y manejar outliers es esencial para la calidad de nuestros análisis. Además, la imputación de valores faltantes nos permite mantener la integridad del dataset para análisis posteriores.
"""

# Suponiendo que 'dataset_news' es tu DataFrame y 'cant_caracteres' es la columna con la cantidad de caracteres
# Crear una columna de índice para el gráfico de dispersión
import plotly.express as px
dataset_news.reset_index(inplace=True)

# Histograma
fig_hist = px.histogram(dataset_news, x='cant_caracteres', nbins=50, title='Distribución de la cantidad de caracteres en las noticias')
fig_hist.show()

# # Gráfico de dispersión
fig_scatter = px.scatter(dataset_news, x='index', y='cant_caracteres', title='Cantidad de caracteres en las noticias')
fig_scatter.show()

# # Violin plot
fig_violin = px.violin(dataset_news, y='cant_caracteres', box=True, points='all', title='Violin plot de la cantidad de caracteres en las noticias')
fig_violin.show()

# # Boxplot con todos los puntos
fig_box = px.box(dataset_news, y='cant_caracteres', points='all', title='Boxplot con todos los puntos')
fig_box.show()

# Boxplot para el largo de las noticias
import plotly.express as px
fig_cant_caract = px.box(dataset_news, y="cant_caracteres", title="Boxplot de cantidad de caracteres por noticias")
fig_cant_caract.show()

"""## Detección

Los outliers pueden distorsionar los resultados del análisis estadístico y los modelos predictivos. Es crucial detectar y decidir cómo manejar estos valores extremos. Analizaremos los outliers en las tarifas pagadas por los pasajeros y discutiremos métodos para su tratamiento.

"""

# Revisar el dataset antes de eliminar outliers, analizamos la cantidad de caracteres VER SI BORRAMOS LOS CASOS OUTLIERS
print(dataset_news['cant_caracteres'].describe())

dataset_news.groupby('label')['cant_caracteres'].describe()

# Boxplot para la cantidad de caracteres por cada tipo de noticia '
import plotly.express as px
fig_pclass = px.box(dataset_news, x="label", y="cant_caracteres",
                    title="Boxplot de caracteres por tipo de noticia para Detectar Outliers")
fig_pclass.show()

import plotly.express as px
fig_pclass = px.violin(dataset_news, x="label", y="cant_caracteres",
                    title="Violin Plot de caracteres por tipo de noticia para Detectar Outliers")
fig_pclass.show()

import plotly.express as px
fig_scatter = px.scatter(dataset_news, x='index', y='cant_caracteres', color='label', title='Cantidad de caracteres en las noticias por categoria')
fig_scatter.show()

import plotly.express as px

# Select the data for the specific label
label_data1 = dataset_news[dataset_news['label'] == 'politic']
label_data2 = dataset_news[dataset_news['label'] == 'science']
label_data3= dataset_news[dataset_news['label'] == 'technology']

# Create a scatter plot for the selected label
fig1 = px.scatter(label_data1, x='index', y='cant_caracteres',
                 title='Cantidad de caracteres en las noticias de politic')
fig2 = px.scatter(label_data2, x='index', y='cant_caracteres',
                 title='Cantidad de caracteres en las noticias de science')
fig3 = px.scatter(label_data3, x='index', y='cant_caracteres',
                 title='Cantidad de caracteres en las noticias de technology')

fig1.show()
fig2.show()
fig3.show()

"""# **Histograma de la hora del día: Este gráfico te mostrará la distribución de las noticias en diferentes horas del día.**"""

# Extraer la hora del día
dataset_news['hour'] = dataset_news['date'].dt.hour

# Crear el histograma
plt.figure(figsize=(10, 6))
plt.hist(dataset_news['hour'], bins=24, color='skyblue', edgecolor='black')
plt.title('Distribución de noticias por hora del día')
plt.xlabel('Hora del día')
plt.ylabel('Cantidad de noticias')
plt.xticks(range(0, 24))
plt.grid(True)
plt.show()

"""# Conclusiones Generales del Análisis Exploratorio de Datos (EDA)
Resumen del Análisis Exploratorio de Datos (EDA) y Conclusiones:

Inspección Inicial y Limpieza de Datos:

Se realizó una inspección inicial del dataset del dataset_news para identificar problemas comunes como valores faltantes y tipos de datos incorrectos.
Se observaron 18 valores faltantes en la columna de texto, lo que no nos sugiere la necesidad de imputación optamos por la eliminación de estos registros.

Exploración de Datos Numéricos:

Se visualizó la distribución de las noticias por tipo a través de un histograma, utilizando Plotly Express, lo que permitió identificar la cantidad de noticias en cada categoría.
Se examinó la cantidad de noticias por día de la semana y por tipo de noticia, así como por mes, revelando patrones interesantes de distribución temporal.
Detección de Outliers y Imputación de Valores Faltantes:

Se utilizó un boxplot para identificar outliers en la cantidad de caracteres por noticia, lo que ayudó a detectar valores atípicos que podrían distorsionar el análisis.

Visualización Adicional:

Se exploró la distribución de noticias por hora del día a través de un histograma, proporcionando información sobre los patrones de publicación de noticias durante el día.

Conclusiones Generales del Análisis Exploratorio de Datos (EDA):

Esta exploracion nos permitio comprender las características de los datos, detectar errores y anomalías, explorar relaciones entre variables y formular hipótesis.

# **DIVIDIR EL DATASET**
"""

# Convertir todos los valores a cadenas de texto porque sino me da error en las funciones de analisis de palabras
dataset_news['text'] = dataset_news['text'].astype(str)

# Dividir el dataset en conjunto de entrenamiento (70%) y un conjunto temporal (30%)
train_df, temp_df = train_test_split(dataset_news, test_size=0.3, random_state=42)

# Dividir el conjunto temporal en validación (50% del temporal, es decir, 15% del total) y prueba (50% del temporal, es decir, 15% del total)
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)

# Agregar una columna 'split' a cada DataFrame
train_df['split'] = 'train'
val_df['split'] = 'val'
test_df['split'] = 'test'

# Unir los DataFrames nuevamente
dataset_news = pd.concat([train_df, val_df, test_df])

# Verificar la distribución de los conjuntos
print(dataset_news['split'].value_counts())

# Mostrar las primeras filas del dataset entrenamiento train_df
train_df.sample(5)

"""# **ver cuantos datos de cada categoria tiene el df de train **"""

# news_count_by_type_train = train_df['label'].value_counts()
# news_count_df_train = pd.DataFrame({'Tipo de Noticia': news_count_by_type.index, 'Cantidad': news_count_by_type_train.values})

# # Crear el gráfico de barras
# plt.figure(figsize=(10, 6))
# sns.barplot(x='Tipo de Noticia', y='Cantidad', data=news_count_df, palette='viridis')
# plt.title('Cantidad de Noticias por Tipo de Noticia')
# plt.xlabel('Tipo de Noticia')
# plt.ylabel('Cantidad de Noticias')
# plt.xticks(rotation=45)
# plt.show()

news_count_by_type_train = train_df['label'].value_counts()
news_count_df_train = pd.DataFrame({'Tipo de Noticia': news_count_by_type_train.index, 'Cantidad': news_count_by_type_train.values})

# Calcular porcentajes
total_count = news_count_df_train['Cantidad'].sum()
news_count_df_train['Porcentaje'] = (news_count_df_train['Cantidad'] / total_count) * 100

# Crear el gráfico de barras
plt.figure(figsize=(10, 6))
sns.barplot(x='Tipo de Noticia', y='Cantidad', data=news_count_df_train, palette='viridis')

# Agregar los porcentajes a las barras
for index, row in news_count_df_train.iterrows():
    plt.text(index, row['Cantidad'], f"{row['Porcentaje']:.2f}%", color='black', ha="center")

plt.title('Cantidad de Noticias por Tipo de Noticia')
plt.xlabel('Tipo de Noticia')
plt.ylabel('Cantidad de Noticias')
plt.xticks(rotation=45)
plt.show()

# Definir la función para visualizar las palabras más comunes ANTES DE LA LIMPIEZA DE CONECTORES
def plot_most_common_words(text_column, n=20):
    text = ' '.join(text_column)
    words = nltk.word_tokenize(text)
    freq_dist = nltk.FreqDist(words)
    words_df = pd.DataFrame({'word': list(freq_dist.keys()), 'count': list(freq_dist.values())})
    most_common_words = words_df.nlargest(columns='count', n=n)
    plt.figure(figsize=(12, 8))
    sns.barplot(data=most_common_words, x='count', y='word')
    plt.xlabel('Count')
    plt.ylabel('Word')
    plt.title('Most Common Words')
    plt.show()

    # Visualización de palabras más comunes en el conjunto de entrenamiento
plot_most_common_words(dataset_news[dataset_news['split'] == 'train']['text'])

"""# Procesamiento de datos"""

from bs4 import BeautifulSoup
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer


# Descargar las stopwords de NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')


# Función para limpiar el texto

def clean_text(text):
    if pd.isnull(text):
        return ""
    # Eliminar las etiquetas HTML
    text = BeautifulSoup(text, "html.parser").get_text()
    # Eliminar caracteres no alfabéticos
    text = re.sub(r'[^a-zA-Z]', ' ', text)
    # Convertir el texto a minúsculas
    text = text.lower()
    # Eliminar stopwords
    stop_words = set(stopwords.words('english'))
    words = word_tokenize(text)
    # Eliminar palabras comunes adicionales
    common_words = ['said', 'one', 'would', 'could', 'also', 'like', 'many', 'new','u', 'us', 'say', 'says']  # Agrega aquí las palabras adicionales que deseas eliminar
    stop_words.update(common_words)
    filtered_words = [word for word in words if word not in stop_words]
    # Lematización
    lemmatizer = WordNetLemmatizer()
    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]
    return ' '.join(lemmatized_words)



# Aplicar la función de limpieza al DataFrame
train_df['clean_text'] = train_df['text'].apply(clean_text)
val_df['clean_text'] = val_df['text'].apply(clean_text)
test_df['clean_text'] = test_df['text'].apply(clean_text)

# Mostrar las transformaciones
print(train_df[['text', 'clean_text']].head())

# visualizamos Información general del dataset con los datos limpios
dataset_news.info()

# Resumen estadístico de las variables numéricas
dataset_news.describe()

# Visualización de palabras más comunes en el conjunto de entrenamiento
plot_most_common_words(train_df['clean_text'])

"""## Word Clouds

Una "nube de palabras" visualiza las palabras más frecuentes de un texto, con el tamaño de cada palabra proporcional a su frecuencia. Podemos obtener una impresión rápida de los temas más discutidos en el dataset.
"""

# Función para generar y mostrar una nube de palabras para una categoría específica

from wordcloud import WordCloud
import matplotlib.pyplot as plt
def generate_wordcloud(text_column, category):
  text = ' '.join(text_column)
  wordcloud = WordCloud(width=800, height=400).generate(text)

  plt.figure(figsize=(10, 5))
  plt.imshow(wordcloud, interpolation='bilinear')
  plt.title(f'Word Cloud for {category} News')
  plt.axis('off')
  plt.show()

    # Generar y mostrar nubes de palabras para cada categoría de noticias en el conjunto de entrenamiento
categories = train_df['label'].unique()
for category in categories:
  category_text = train_df[train_df['label'] == category]['clean_text']
  generate_wordcloud(category_text, category)

"""# Extracción de Características
Convertiremos el texto limpio en formatos numéricos que los modelos de machine learning pueden procesar utilizando diferentes técnicas de vectorización.
"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Ya hemos cargado y limpiado el dataset, y lo divididimos en train_df, val_df y test_df, aplicamos los distintos modelos al train

# Bag of Words
vectorizer_bow = CountVectorizer()
X_bow_train = vectorizer_bow.fit_transform(train_df['clean_text'])
X_bow_test = vectorizer_bow.transform(test_df['clean_text'])

# TF-IDF
vectorizer_tfidf = TfidfVectorizer()
X_tfidf_train = vectorizer_tfidf.fit_transform(train_df['clean_text'])
X_tfidf_test = vectorizer_tfidf.transform(test_df['clean_text'])
X_tfidf_val = vectorizer_tfidf.transform(val_df['clean_text'])

# Comparación de características extraídas
print("Número de características en BoW:", X_bow_train.shape[1])
print("Número de características en TF-IDF:", X_tfidf_train.shape[1])

"""## Visualización de características para cada noticia - BoW"""

# Obteniendo nombres de las características (palabras) para BoW
feature_names_bow = vectorizer_bow.get_feature_names_out()

# Convertir una fila de la matriz BoW a DataFrame para una visualización más fácil
def bow_to_df(row_index):
    row = X_bow_train[row_index].toarray().flatten()  # Convierte a array y aplana
    df = pd.DataFrame({'Word': feature_names_bow, 'Frequency': row})
    return df[df['Frequency'] > 0]  # Filtrar por frecuencias mayores a cero

# Visualizar la conversión BoW de una revisión específica
example_bow = bow_to_df(0)  # Cambia 0 por otro índice si es necesario
print(train_df['clean_text'].iloc[0])
example_bow

"""# Visualización de características para cada noticia - TF-IDF"""

# Convertir una fila de la matriz TF-IDF a DataFrame para una visualización más fácil

feature_names_tfidf = vectorizer_tfidf.get_feature_names_out()
def tfidf_to_df(row_index):
    row = X_tfidf_train[row_index].toarray().flatten()  # Convierte a array y aplana
    df = pd.DataFrame({'Word': feature_names_tfidf, 'TF-IDF Value': row})
    return df[df['TF-IDF Value'] > 0]  # Filtrar por valores TF-IDF mayores a cero

# Visualizar la conversión TF-IDF de una revisión específica
example_tfidf = tfidf_to_df(0)  # Cambia 0 por otro índice si es necesario
print(train_df['clean_text'].iloc[0])
example_tfidf

"""## Visualización de distribución de palabras - BoW"""

# Función para graficar las palabras más frecuentes de todo el dataset limpio
def plot_top_words(vectorizer, feature_matrix, top_n=20):
    sum_words = feature_matrix.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)[:top_n]
    df = pd.DataFrame(words_freq, columns=['Word', 'Frequency'])
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Frequency', y='Word', data=df, palette='PuBuGn_d')
    plt.title('Top Words in Corpus')
    plt.show()

# Ejemplo de uso con BoW
plot_top_words(vectorizer_bow, X_bow_train)

# Función para graficar las palabras más frecuentes por categoría
def plot_top_words_by_category(vectorizer, feature_matrix, categories, category_name, top_n=20):
    # Filtrar las filas correspondientes a la categoría especificada
    category_indices = [i for i, cat in enumerate(categories) if cat == category_name]
    category_matrix = feature_matrix[category_indices]

    # Sumar las ocurrencias de cada palabra en los documentos de la categoría
    sum_words = category_matrix.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)[:top_n]

    # Crear un DataFrame para las palabras más frecuentes
    df = pd.DataFrame(words_freq, columns=['Word', 'Frequency'])

    # Graficar las palabras más frecuentes
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Frequency', y='Word', data=df, palette='PuBuGn_d')
    plt.title(f'Top Words in {category_name.capitalize()}')
    plt.show()

# Aplicar la función para cada categoría
categories = train_df['label'].tolist()
plot_top_words_by_category(vectorizer_bow, X_bow_train, categories, 'politic')
plot_top_words_by_category(vectorizer_bow, X_bow_train, categories, 'science')
plot_top_words_by_category(vectorizer_bow, X_bow_train, categories, 'technology')

"""# Visualización de distribución de palabras - TF-IDF"""

import matplotlib.pyplot as plt
import seaborn as sns

def plot_top_words(vectorizer, feature_matrix, top_n=20):
    sum_words = feature_matrix.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)[:top_n]
    df = pd.DataFrame(words_freq, columns=['Word', 'Frequency'])
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Frequency', y='Word', data=df, palette='PuBuGn_d')
    plt.title('Top Words in Corpus')
    plt.show()

# Ejemplo de uso con TF-IDF
plot_top_words(vectorizer_bow, X_tfidf_train)

"""## Entrenamiento de modelo de regresión logística basado en BoW"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Entrenamiento del modelo con BoW
model = LogisticRegression()
model.fit(X_bow_train, train_df['label'])

# Evaluación del modelo con BoW
predictions = model.predict(X_bow_test)

# Reporte de clasificación
print(classification_report(test_df['label'], predictions))

"""## Entrenamiento de modelo de regresión logística basado en TFIDF"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Entrenamiento del modelo
model = LogisticRegression()
model.fit(X_tfidf_train, train_df['label'])  # Donde'label' es la etiqueta de categorización de la noticia

# Evaluación del modelo usando el conjunto de validación
predictions = model.predict(X_tfidf_val)
print(classification_report(val_df['label'], predictions))

# Evaluación del modelo
predictions = model.predict(X_tfidf_test)
print(classification_report(test_df['label'], predictions))

"""# Validación"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.feature_extraction.text import TfidfVectorizer

# Vectorización de los datos de validación y prueba
X_tfidf_val = vectorizer_tfidf.transform(val_df['clean_text'])
X_tfidf_test = vectorizer_tfidf.transform(test_df['clean_text'])

# Entrenamiento del modelo con el conjunto de entrenamiento
model = LogisticRegression(max_iter=1000)  # Aumentamos el número de iteraciones para asegurar la convergencia
model.fit(X_tfidf_train, train_df['label'])

# Evaluación del modelo usando el conjunto de validación
predictions_val = model.predict(X_tfidf_val)
print("Resultados en el conjunto de validación:")
print(classification_report(val_df['label'], predictions_val))

# Evaluación del modelo usando el conjunto de prueba
predictions_test = model.predict(X_tfidf_test)
print("Resultados en el conjunto de prueba:")
print(classification_report(test_df['label'], predictions_test))

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Predicciones del modelo
predictions = model.predict(X_tfidf_test)

# Calculo de la matriz de confusión
cm = confusion_matrix(test_df['label'], predictions)

# Visualización de la matriz de confusión
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)
plt.xlabel('Predicción')
plt.ylabel('Actual')
plt.title('Matriz de Confusión')
plt.show()

"""Redes Neuronales

# Modelo RNN
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

# Preparación de datos
tokenizer = Tokenizer(num_words=50000)  # Mantener un vocabulario amplio
tokenizer.fit_on_texts(train_df['clean_text'])
sequences = tokenizer.texts_to_sequences(train_df['clean_text'])
X_train = pad_sequences(sequences, maxlen=250)  # Ajustado a 250 caracteres para cubrir la mayoría de los documentos

# Codificación de etiquetas a formato categórico
y_train = to_categorical(train_df['clasificacion'] - 1, num_classes=3)  #  restar 1, empieza en 1

# Construcción del modelo RNN
model_rnn = Sequential()
model_rnn.add(Embedding(50000, 100, input_length=250))  # Embedding
model_rnn.add(SimpleRNN(50)) # Cantidad de neuronas de la capa
model_rnn.add(Dense(3, activation='softmax'))  # Tres categorías de salida
model_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model_rnn.summary()

# Entrenamiento del modelo
model_rnn.fit(X_train, y_train, epochs=5, batch_size=64)  # Entrenamiento del modelo

"""# Modelo LSTM"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

# Preparación de datos
tokenizer = Tokenizer(num_words=10000) # Mantener un vocabulario amplio
tokenizer.fit_on_texts(train_df['clean_text']) # Ajustado a 300 caracteres para cubrir la mayoría de los documentos
sequences = tokenizer.texts_to_sequences(train_df['clean_text'])
X_train = pad_sequences(sequences, maxlen=250)

# Codificación de etiquetas a formato categórico
y_train = to_categorical(train_df['clasificacion'] - 1, num_classes=3)  #  restar 1, empieza en 1

# Construcción del modelo LSTM
model_lstm = Sequential()
model_lstm.add(Embedding(50000, 100, input_length=250))
model_lstm.add(LSTM(50))  # Cantidad de neuronas de la capa
model_lstm.add(Dense(3, activation='softmax'))  # Tres categorías de salida
model_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model_lstm.summary()

# Entrenamiento del modelo
model_lstm.fit(X_train, y_train, epochs=5, batch_size=64) # Entrenamiento del modelo

"""Comparación de Modelos RNN -LSTM


"""

from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix
from tensorflow.keras.utils import to_categorical

# Función para calcular métricas de evaluación
def evaluate_model(model, X_test, y_test):
    predictions = model.predict(X_test)
    predictions = predictions.argmax(axis=1)  # Convertir las probabilidades a clases (ver)
    y_test_classes = y_test.argmax(axis=1)    # Convertir one-hot a clases(ver)
    accuracy = accuracy_score(y_test_classes, predictions)
    recall = recall_score(y_test_classes, predictions, average='weighted')
    f1 = f1_score(y_test_classes, predictions, average='weighted')
    conf_matrix = confusion_matrix(y_test_classes, predictions)
    return accuracy, recall, f1, conf_matrix

# Preparación de datos de prueba
sequences_test = tokenizer.texts_to_sequences(test_df['clean_text'])
X_test = pad_sequences(sequences_test, maxlen=250)
y_test = to_categorical(test_df['clasificacion'] - 1, num_classes=3)

# Evaluar RNN
accuracy_rnn, recall_rnn, f1_rnn, conf_matrix_rnn = evaluate_model(model_rnn, X_test, y_test)
# Evaluar LSTM
accuracy_lstm, recall_lstm, f1_lstm, conf_matrix_lstm = evaluate_model(model_lstm, X_test, y_test)

# Imprimir los resultados
print("RNN Model - Accuracy: {:.2f}, Recall: {:.2f}, F1 Score: {:.2f}".format(accuracy_rnn, recall_rnn, f1_rnn))
print("LSTM Model - Accuracy: {:.2f}, Recall: {:.2f}, F1 Score: {:.2f}".format(accuracy_lstm, recall_lstm, f1_lstm))

# Imprimir las matrices de confusión
print("Confusion Matrix for RNN Model:\n", conf_matrix_rnn)
print("Confusion Matrix for LSTM Model:\n", conf_matrix_lstm)

"""# **Extracción de modelo TF-IDF**"""

import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Suponiendo que 'vectorizer_tfidf' es tu vectorizador TF-IDF y 'model' es tu modelo entrenado
joblib.dump(vectorizer_tfidf, '/content/drive/My Drive/tfidf_vectorizer.pkl')
joblib.dump(model, '/content/drive/My Drive/logistic_regression_model.pkl')

print("Modelos guardados exitosamente en Google Drive.")



import joblib
from google.colab import drive

# Montar Google Drive
drive.mount('/content/drive')

# Rutas de archivo en Google Drive
vectorizer_path = '/content/drive/My Drive/vectorizer5.pkl'
model_path = '/content/drive/My Drive/model5.pkl'

# Guardar los archivos en Google Drive
joblib.dump(vectorizer_tfidf, vectorizer_path)
joblib.dump(model, model_path)